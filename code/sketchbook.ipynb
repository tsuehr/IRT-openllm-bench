{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from huggingface_hub.utils import build_hf_headers\n",
    "import os\n",
    "import json\n",
    "with open(\"./../token.txt\",\"r\") as f:\n",
    "    HF_TOKEN = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "def collect_model_names(directory):\n",
    "    model_names = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                #print(file_path)\n",
    "                with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    if \"model_name_sanitized\" in data:\n",
    "                        model_names.append(data[\"model_name_sanitized\"])\n",
    "                \n",
    "\n",
    "    return model_names\n",
    "\n",
    "# Example usage\n",
    "directory = './../results/'  # Replace with the path to your directory\n",
    "model_names = collect_model_names(directory)\n",
    "model_names = list(set(model_names))\n",
    "print(len(model_names))\n",
    "# with open('./../results/01-ai/Yi-1.5-6B/results_2024-06-17T14-53-45.170909.json','r',encoding='utf-8') as json_file:\n",
    "#     data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_access_to_gated_repo(model):\n",
    "    headers = build_hf_headers(token=HF_TOKEN)\n",
    "    # or directly `headers = {\"authorization\": \"Bearer hf_xxx\"}`\n",
    "\n",
    "    repo_id = \"open-llm-leaderboard/\"+model+\"-details\"\n",
    "    repo_type = \"dataset\"\n",
    "    url = f\"https://huggingface.co/{repo_type}s/{repo_id}/ask-access\"\n",
    "\n",
    "    response = requests.post(url, headers=headers)\n",
    "    response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databricks__dbrx-instruct\n",
      "mistralai__Mixtral-8x22B-Instruct-v0.1\n",
      "NousResearch__Yarn-Llama-2-13b-128k\n"
     ]
    }
   ],
   "source": [
    "#get access to all models in list\n",
    "for model in model_names:\n",
    "    try:\n",
    "        get_access_to_gated_repo(model)\n",
    "    except:\n",
    "        print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 73.0k/73.0k [00:00<00:00, 72.5MB/s]\n",
      "Downloading data: 100%|██████████| 8.23M/8.23M [00:00<00:00, 8.64MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 1172 examples [00:00, 41089.01 examples/s]\n",
      "Generating latest split: 1172 examples [00:00, 47340.78 examples/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:02<00:00,  8.76files/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:00<00:00, 24007.46files/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 437 examples [00:00, 15748.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: stabilityai__stablelm-2-12b-chat/leaderboard_bbh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 30115.05 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16884.19 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 187 examples [00:00, 15756.97 examples/s]\n",
      "Generating latest split: 187 examples [00:00, 15686.07 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 20529.72 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 24618.51 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 21678.68 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 20555.07 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 23413.03 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 21178.22 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 14474.10 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 14332.84 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 27121.62 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22725.47 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 18190.86 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 19328.59 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 15276.90 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 15679.17 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 22410.26 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 24482.85 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 21173.09 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 20824.09 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 25580.02 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 27403.01 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 12373.45 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 12504.63 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 146 examples [00:00, 12447.78 examples/s]\n",
      "Generating latest split: 146 examples [00:00, 12389.60 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 8749.22 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 9095.75 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 18891.22 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22793.15 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 12223.73 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 12505.23 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 178 examples [00:00, 18035.66 examples/s]\n",
      "Generating latest split: 178 examples [00:00, 22200.00 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 29985.02 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 30551.13 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 15194.55 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 19342.85 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 14717.06 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16448.25 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 10121.78 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 12187.36 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 18549.35 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 15781.35 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 18229.45 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 23674.70 examples/s]\n",
      "Downloading data: 100%|██████████| 2.73M/2.73M [00:00<00:00, 5.00MB/s]\n",
      "Downloading data: 100%|██████████| 8.03M/8.03M [00:00<00:00, 16.7MB/s]\n",
      "Downloading data: 100%|██████████| 6.33M/6.33M [00:00<00:00, 13.7MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 198 examples [00:00, 3607.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: stabilityai__stablelm-2-12b-chat/leaderboard_gpqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_16T18_22_44.958860 split: 198 examples [00:00, 7249.93 examples/s]\n",
      "Generating latest split: 198 examples [00:00, 9328.32 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 546 examples [00:00, 13040.85 examples/s]\n",
      "Generating latest split: 546 examples [00:00, 13707.29 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 448 examples [00:00, 13706.18 examples/s]\n",
      "Generating latest split: 448 examples [00:00, 13179.92 examples/s]\n",
      "Downloading data: 100%|██████████| 3.37M/3.37M [00:00<00:00, 9.38MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 541 examples [00:00, 32343.44 examples/s]\n",
      "Generating latest split: 541 examples [00:00, 32217.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: stabilityai__stablelm-2-12b-chat/leaderboard_math_hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.66M/1.66M [00:00<00:00, 5.90MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 307 examples [00:00, 23206.36 examples/s]\n",
      "Generating latest split: 307 examples [00:00, 21161.07 examples/s]\n",
      "Downloading data: 100%|██████████| 727k/727k [00:00<00:00, 2.63MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 123 examples [00:00, 10112.11 examples/s]\n",
      "Generating latest split: 123 examples [00:00, 7172.44 examples/s]\n",
      "Downloading data: 100%|██████████| 889k/889k [00:00<00:00, 2.11MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 132 examples [00:00, 11288.57 examples/s]\n",
      "Generating latest split: 132 examples [00:00, 10638.69 examples/s]\n",
      "Downloading data: 100%|██████████| 1.75M/1.75M [00:00<00:00, 2.89MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 280 examples [00:00, 17184.24 examples/s]\n",
      "Generating latest split: 280 examples [00:00, 20435.81 examples/s]\n",
      "Downloading data: 100%|██████████| 860k/860k [00:00<00:00, 3.08MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 154 examples [00:00, 15239.06 examples/s]\n",
      "Generating latest split: 154 examples [00:00, 14359.58 examples/s]\n",
      "Downloading data: 100%|██████████| 1.01M/1.01M [00:00<00:00, 2.62MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 193 examples [00:00, 15393.84 examples/s]\n",
      "Generating latest split: 193 examples [00:00, 14464.93 examples/s]\n",
      "Downloading data: 100%|██████████| 987k/987k [00:00<00:00, 4.01MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 135 examples [00:00, 8586.28 examples/s]\n",
      "Generating latest split: 135 examples [00:00, 10179.80 examples/s]\n",
      "Downloading data: 100%|██████████| 375M/375M [00:10<00:00, 34.9MB/s] \n",
      "Generating 2024_06_16T18_22_44.958860 split: 12032 examples [00:01, 9175.07 examples/s] \n",
      "Generating latest split: 12032 examples [00:08, 1433.39 examples/s]\n",
      "Downloading data: 100%|██████████| 4.45M/4.45M [00:00<00:00, 8.25MB/s]\n",
      "Downloading data: 100%|██████████| 6.49M/6.49M [00:00<00:00, 8.69MB/s]\n",
      "Downloading data: 100%|██████████| 3.79M/3.79M [00:00<00:00, 8.70MB/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 7827.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: stabilityai__stablelm-2-12b-chat/leaderboard_musr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 16834.32 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 15637.08 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 256 examples [00:00, 12498.74 examples/s]\n",
      "Generating latest split: 256 examples [00:00, 12800.78 examples/s]\n",
      "Generating 2024_06_16T18_22_44.958860 split: 250 examples [00:00, 20159.11 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 18165.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_arc_challenge\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_boolean_expressions\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_causal_judgement\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_date_understanding\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_disambiguation_qa\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_formal_fallacies\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_geometric_shapes\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_hyperbaton\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_logical_deduction_five_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_logical_deduction_seven_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_logical_deduction_three_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_movie_recommendation\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_navigate\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_object_counting\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_penguins_in_a_table\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_reasoning_about_colored_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_ruin_names\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_salient_translation_error_detection\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_snarks\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_sports_understanding\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_temporal_sequences\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_tracking_shuffled_objects_five_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_tracking_shuffled_objects_seven_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_tracking_shuffled_objects_three_objects\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_bbh_web_of_lies\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_gpqa\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_gpqa_diamond\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_gpqa_extended\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_gpqa_main\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_ifeval\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_algebra_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_counting_and_prob_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_geometry_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_intermediate_algebra_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_num_theory_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_prealgebra_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_math_precalculus_hard\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_mmlu_pro\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_musr\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_musr_murder_mysteries\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_musr_object_placements\n",
      "skipped: vicgalle__Roleplay-Llama-3-8B/leaderboard_musr_team_allocation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 73.3k/73.3k [00:00<00:00, 15.3MB/s]\n",
      "Downloading data: 100%|██████████| 6.71M/6.71M [00:00<00:00, 16.6MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 1172 examples [00:00, 65115.83 examples/s]\n",
      "Generating latest split: 1172 examples [00:00, 65109.79 examples/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:02<00:00,  9.30files/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:00<?, ?files/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 437 examples [00:00, 18207.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: mistralai__Mistral-7B-Instruct-v0.3/leaderboard_bbh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 22715.63 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 18493.73 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 187 examples [00:00, 12295.96 examples/s]\n",
      "Generating latest split: 187 examples [00:00, 15585.70 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 20620.15 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 21864.46 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 22988.03 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 23342.15 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 24738.73 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22204.77 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 16391.94 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 18729.25 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 24997.04 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 27771.70 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 20578.88 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 20224.04 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 14468.11 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 18589.47 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 24245.09 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 24286.08 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 22811.50 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 21770.05 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 25005.39 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 26587.96 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 15011.18 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 14571.04 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 146 examples [00:00, 13114.78 examples/s]\n",
      "Generating latest split: 146 examples [00:00, 14430.40 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 9272.62 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 10751.54 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 20413.02 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 23503.82 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 11289.94 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 14059.75 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 178 examples [00:00, 17797.05 examples/s]\n",
      "Generating latest split: 178 examples [00:00, 17435.05 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 10460.03 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 27665.45 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 17620.16 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 19740.13 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 21778.64 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 20846.44 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 15469.83 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16238.87 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 21749.73 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 20914.22 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 9882.90 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 24572.93 examples/s]\n",
      "Downloading data: 100%|██████████| 2.65M/2.65M [00:00<00:00, 9.09MB/s]\n",
      "Downloading data: 100%|██████████| 7.82M/7.82M [00:00<00:00, 18.6MB/s]\n",
      "Downloading data: 100%|██████████| 6.16M/6.16M [00:00<00:00, 13.3MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 198 examples [00:00, 3952.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: mistralai__Mistral-7B-Instruct-v0.3/leaderboard_gpqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_17T14_37_59.603167 split: 198 examples [00:00, 6349.52 examples/s]\n",
      "Generating latest split: 198 examples [00:00, 11437.59 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 546 examples [00:00, 17766.55 examples/s]\n",
      "Generating latest split: 546 examples [00:00, 18892.95 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 448 examples [00:00, 15191.35 examples/s]\n",
      "Generating latest split: 448 examples [00:00, 14392.11 examples/s]\n",
      "Downloading data: 100%|██████████| 2.67M/2.67M [00:00<00:00, 4.21MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 541 examples [00:00, 38699.04 examples/s]\n",
      "Generating latest split: 541 examples [00:00, 43829.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: mistralai__Mistral-7B-Instruct-v0.3/leaderboard_math_hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.68M/1.68M [00:00<00:00, 5.98MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 307 examples [00:00, 26396.03 examples/s]\n",
      "Generating latest split: 307 examples [00:00, 22216.97 examples/s]\n",
      "Downloading data: 100%|██████████| 715k/715k [00:00<00:00, 2.78MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 123 examples [00:00, 11408.66 examples/s]\n",
      "Generating latest split: 123 examples [00:00, 9619.43 examples/s]\n",
      "Downloading data: 100%|██████████| 856k/856k [00:00<00:00, 3.12MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 132 examples [00:00, 13149.85 examples/s]\n",
      "Generating latest split: 132 examples [00:00, 10662.87 examples/s]\n",
      "Downloading data: 100%|██████████| 1.93M/1.93M [00:00<00:00, 5.68MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 280 examples [00:00, 22583.85 examples/s]\n",
      "Generating latest split: 280 examples [00:00, 25456.39 examples/s]\n",
      "Downloading data: 100%|██████████| 895k/895k [00:00<00:00, 3.31MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 154 examples [00:00, 16626.07 examples/s]\n",
      "Generating latest split: 154 examples [00:00, 11134.68 examples/s]\n",
      "Downloading data: 100%|██████████| 951k/951k [00:00<00:00, 3.44MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 193 examples [00:00, 21258.45 examples/s]\n",
      "Generating latest split: 193 examples [00:00, 14703.76 examples/s]\n",
      "Downloading data: 100%|██████████| 1.06M/1.06M [00:00<00:00, 1.99MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 135 examples [00:00, 13343.18 examples/s]\n",
      "Generating latest split: 135 examples [00:00, 11273.66 examples/s]\n",
      "Downloading data: 100%|██████████| 338M/338M [00:09<00:00, 36.8MB/s] \n",
      "Generating 2024_06_17T14_37_59.603167 split: 12032 examples [00:00, 14784.16 examples/s]\n",
      "Generating latest split: 12032 examples [00:06, 1813.07 examples/s]\n",
      "Downloading data: 100%|██████████| 4.40M/4.40M [00:00<00:00, 10.1MB/s]\n",
      "Downloading data: 100%|██████████| 6.40M/6.40M [00:00<00:00, 12.6MB/s]\n",
      "Downloading data: 100%|██████████| 3.72M/3.72M [00:00<00:00, 9.34MB/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 9032.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: mistralai__Mistral-7B-Instruct-v0.3/leaderboard_musr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 17676.60 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16512.49 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 256 examples [00:00, 14321.90 examples/s]\n",
      "Generating latest split: 256 examples [00:00, 15616.25 examples/s]\n",
      "Generating 2024_06_17T14_37_59.603167 split: 250 examples [00:00, 8303.32 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22004.41 examples/s]\n",
      "Downloading readme: 100%|██████████| 73.2k/73.2k [00:00<00:00, 14.8MB/s]\n",
      "Downloading data: 100%|██████████| 7.45M/7.45M [00:00<00:00, 10.2MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 1172 examples [00:00, 56720.33 examples/s]\n",
      "Generating latest split: 1172 examples [00:00, 25933.52 examples/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:02<00:00,  8.01files/s]\n",
      "Downloading data: 100%|██████████| 24/24 [00:00<?, ?files/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 437 examples [00:00, 12002.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: deepseek-ai__deepseek-llm-67b-chat/leaderboard_bbh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 24669.47 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 24227.73 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 187 examples [00:00, 14114.87 examples/s]\n",
      "Generating latest split: 187 examples [00:00, 17847.29 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 20995.45 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 19620.46 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 22058.57 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 23684.32 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 23898.08 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22089.70 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 14582.39 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 13317.96 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 24608.11 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 30203.53 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 19631.48 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 21073.94 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 16368.91 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16729.30 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 20306.68 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 25226.16 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 24675.86 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22225.95 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 27223.01 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 27223.01 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 14260.13 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 15225.88 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 146 examples [00:00, 13770.06 examples/s]\n",
      "Generating latest split: 146 examples [00:00, 13768.20 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 9921.80 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 10312.51 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 20051.55 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 22406.91 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 11583.92 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 10759.48 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 178 examples [00:00, 18245.91 examples/s]\n",
      "Generating latest split: 178 examples [00:00, 17206.81 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 23688.60 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 25918.28 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 17674.22 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 17976.31 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 20098.83 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 16797.37 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 11294.20 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 12735.02 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 16659.93 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 17799.32 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 250 examples [00:00, 17969.22 examples/s]\n",
      "Generating latest split: 250 examples [00:00, 17940.63 examples/s]\n",
      "Downloading data: 100%|██████████| 2.66M/2.66M [00:00<00:00, 8.94MB/s]\n",
      "Downloading data: 100%|██████████| 7.85M/7.85M [00:00<00:00, 17.9MB/s]\n",
      "Downloading data: 100%|██████████| 6.18M/6.18M [00:01<00:00, 5.77MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 198 examples [00:00, 4690.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: deepseek-ai__deepseek-llm-67b-chat/leaderboard_gpqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2024_06_26T06_16_32.984312 split: 198 examples [00:00, 8710.27 examples/s]\n",
      "Generating latest split: 198 examples [00:00, 8628.55 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 546 examples [00:00, 12162.63 examples/s]\n",
      "Generating latest split: 546 examples [00:00, 19560.88 examples/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 448 examples [00:00, 17146.32 examples/s]\n",
      "Generating latest split: 448 examples [00:00, 17634.21 examples/s]\n",
      "Downloading data: 100%|██████████| 2.37M/2.37M [00:00<00:00, 3.68MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 541 examples [00:00, 34570.72 examples/s]\n",
      "Generating latest split: 541 examples [00:00, 34321.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: deepseek-ai__deepseek-llm-67b-chat/leaderboard_math_hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.58M/1.58M [00:00<00:00, 3.91MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 307 examples [00:00, 26741.39 examples/s]\n",
      "Generating latest split: 307 examples [00:00, 20834.77 examples/s]\n",
      "Downloading data: 100%|██████████| 687k/687k [00:00<00:00, 2.79MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 123 examples [00:00, 11580.23 examples/s]\n",
      "Generating latest split: 123 examples [00:00, 11365.93 examples/s]\n",
      "Downloading data: 100%|██████████| 881k/881k [00:00<00:00, 3.44MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 132 examples [00:00, 14115.75 examples/s]\n",
      "Generating latest split: 132 examples [00:00, 10522.03 examples/s]\n",
      "Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 6.74MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 280 examples [00:00, 24916.83 examples/s]\n",
      "Generating latest split: 280 examples [00:00, 20846.44 examples/s]\n",
      "Downloading data: 100%|██████████| 825k/825k [00:00<00:00, 2.65MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 154 examples [00:00, 15576.79 examples/s]\n",
      "Generating latest split: 154 examples [00:00, 12105.45 examples/s]\n",
      "Downloading data: 100%|██████████| 958k/958k [00:00<00:00, 3.60MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 193 examples [00:00, 15150.11 examples/s]\n",
      "Generating latest split: 193 examples [00:00, 14628.83 examples/s]\n",
      "Downloading data: 100%|██████████| 1.02M/1.02M [00:00<00:00, 2.00MB/s]\n",
      "Generating 2024_06_26T06_16_32.984312 split: 135 examples [00:00, 12032.11 examples/s]\n",
      "Generating latest split: 135 examples [00:00, 11052.94 examples/s]\n",
      "Downloading data: 100%|██████████| 357M/357M [00:08<00:00, 40.0MB/s] \n",
      "Generating 2024_06_26T06_16_32.984312 split: 12032 examples [00:00, 15029.31 examples/s]\n",
      "Generating latest split: 12032 examples [00:08, 1431.13 examples/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 66\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../item_level_results/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmodel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msplit\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes into a single dataframe\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# combined_df = pd.concat(all_dataframes, ignore_index=True)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# combined_df[\"model_names\"] = models\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# combined_df[\"task\"] = tasks\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# combined_df.to_csv('memory.csv', index=False)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# models = models + len(df)*[model]\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# tasks = tasks + len(df) * [split]\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m---> 65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./../item_level_results/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes into a single dataframe\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# combined_df = pd.concat(all_dataframes, ignore_index=True)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# combined_df[\"model_names\"] = models\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# combined_df[\"task\"] = tasks\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# combined_df.to_csv('memory.csv', index=False)\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "splits = ['leaderboard_arc_challenge',\n",
    " 'leaderboard_bbh',\n",
    " 'leaderboard_bbh_boolean_expressions',\n",
    " 'leaderboard_bbh_causal_judgement',\n",
    " 'leaderboard_bbh_date_understanding',\n",
    " 'leaderboard_bbh_disambiguation_qa',\n",
    " 'leaderboard_bbh_formal_fallacies',\n",
    " 'leaderboard_bbh_geometric_shapes',\n",
    " 'leaderboard_bbh_hyperbaton',\n",
    " 'leaderboard_bbh_logical_deduction_five_objects',\n",
    " 'leaderboard_bbh_logical_deduction_seven_objects',\n",
    " 'leaderboard_bbh_logical_deduction_three_objects',\n",
    " 'leaderboard_bbh_movie_recommendation',\n",
    " 'leaderboard_bbh_navigate',\n",
    " 'leaderboard_bbh_object_counting',\n",
    " 'leaderboard_bbh_penguins_in_a_table',\n",
    " 'leaderboard_bbh_reasoning_about_colored_objects',\n",
    " 'leaderboard_bbh_ruin_names',\n",
    " 'leaderboard_bbh_salient_translation_error_detection',\n",
    " 'leaderboard_bbh_snarks',\n",
    " 'leaderboard_bbh_sports_understanding',\n",
    " 'leaderboard_bbh_temporal_sequences',\n",
    " 'leaderboard_bbh_tracking_shuffled_objects_five_objects',\n",
    " 'leaderboard_bbh_tracking_shuffled_objects_seven_objects',\n",
    " 'leaderboard_bbh_tracking_shuffled_objects_three_objects',\n",
    " 'leaderboard_bbh_web_of_lies',\n",
    " 'leaderboard_gpqa',\n",
    " 'leaderboard_gpqa_diamond',\n",
    " 'leaderboard_gpqa_extended',\n",
    " 'leaderboard_gpqa_main',\n",
    " 'leaderboard_ifeval',\n",
    " 'leaderboard_math_hard',\n",
    " 'leaderboard_math_algebra_hard',\n",
    " 'leaderboard_math_counting_and_prob_hard',\n",
    " 'leaderboard_math_geometry_hard',\n",
    " 'leaderboard_math_intermediate_algebra_hard',\n",
    " 'leaderboard_math_num_theory_hard',\n",
    " 'leaderboard_math_prealgebra_hard',\n",
    " 'leaderboard_math_precalculus_hard',\n",
    " 'leaderboard_mmlu_pro',\n",
    " 'leaderboard_musr',\n",
    " 'leaderboard_musr_murder_mysteries',\n",
    " 'leaderboard_musr_object_placements',\n",
    " 'leaderboard_musr_team_allocation']\n",
    "\n",
    "all_dataframes = []\n",
    "models = []\n",
    "tasks = []\n",
    "for model in model_names:\n",
    "    for split in splits:\n",
    "        try:\n",
    "            data = load_dataset(\n",
    "                \"HuggingFaceEvalInternal/\"+model+\"-details-private\",\n",
    "                name=model+\"__\"+split,\n",
    "                split=\"latest\",\n",
    "                token=HF_TOKEN\n",
    "            )\n",
    "        \n",
    "        except:\n",
    "            print(\"skipped: \"+model + \"/\"+split)\n",
    "            continue\n",
    "        # models = models + len(df)*[model]\n",
    "        # tasks = tasks + len(df) * [split]\n",
    "        df = data.to_dict()\n",
    "        with open('./../item_level_results/'+model+\"__\"+split+\".json\",'w') as f:\n",
    "            json.dump(df,f,indent=4)\n",
    "    \n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "        # combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        # combined_df[\"model_names\"] = models\n",
    "        # combined_df[\"task\"] = tasks\n",
    "        # combined_df.to_csv('memory.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
